{"cells":[{"cell_type":"markdown","source":["# Natural Language Processing - Sentiment Analysis Spring 2023\n","\n","#### Team: Priyanka Agarwal (CIS 5190), Karleena Rybacki (CIS 5190), Praveen Raj Uma Maheswari Shyam Sundar (CIS 5190). \n","\n","##### Project Mentor TA: Yiming Huang"],"metadata":{"id":"qgedOI9q-MBU"}},{"cell_type":"markdown","metadata":{"id":"lA-I-8JHq2yF"},"source":["## **Milestone-2**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6EaKiOeqzrp"},"outputs":[],"source":["# Some imports that you might need\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import nltk\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSAEuI9Nq7lL"},"outputs":[],"source":["!pip install --upgrade --no-cache-dir gdown\n","if not os.path.exists(\"Reviews.csv\"):\n","    !gdown 1_kLSwiRYtiXF7h9V1FlTqTapOHiYU5Mk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GDYS4XH9rAUf"},"outputs":[],"source":["# Remeber to change the path here for the corresponding files you need\n","df = pd.read_csv('Reviews.csv')\n","print(df.shape)\n","df_head = df.head(1000)\n","print(df.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-EH3x-IrBag"},"outputs":[],"source":["df = df[['Text', 'Score']]\n","df.dropna(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O614lDNdraAj"},"outputs":[],"source":["#Maping the Scores to binary labels:\n","df['Label'] = df['Score'].apply(lambda x: 'positive' if x >= 4 else 'negative' if x <= 2 else 'neutral')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOEzKhmorcSF"},"outputs":[],"source":["#Dataset Splitting:\n","from sklearn.model_selection import train_test_split\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6As3KppkreNZ"},"outputs":[],"source":["#Evaluation Metrics:\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","def evaluate(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average=\"weighted\")\n","    recall = recall_score(y_true, y_pred, average=\"weighted\")\n","    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n","    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2wIcXhA9rf-I"},"outputs":[],"source":["#Approach One - Logistic Regression\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","\n","vectorizer = CountVectorizer()\n","\n","\n","train_vectors = vectorizer.fit_transform(train_df['Text'])\n","\n","\n","clf = LogisticRegression(max_iter=80)\n","clf.fit(train_vectors, train_df['Label'])\n","\n","\n","val_vectors = vectorizer.transform(val_df['Text'])\n","val_predictions = clf.predict(val_vectors)\n","\n","\n","y_test = val_df['Label']\n","y_pred_lr = val_predictions\n","\n","lr_metrics = evaluate(y_test, y_pred_lr)\n","print(\"Logistic Regression Baseline Metrics: \", lr_metrics)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VIokrV5nriD_"},"outputs":[],"source":["#Approach Two - Support Vector Machine (SVM)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.svm import LinearSVC\n","\n","\n","\n","\n","model_svm = LinearSVC()\n","model_svm.fit(train_vectors, train_df['Label'])\n","\n","\n","y_pred_svm = model_svm.predict(val_vectors)\n","\n","\n","svm_metrics = evaluate(y_test, y_pred_svm)\n","print(\"SVM Metrics: \", svm_metrics)\n"]},{"cell_type":"markdown","metadata":{"id":"QKv2-Exxrp_j"},"source":["## **Milestone-3**"]},{"cell_type":"markdown","metadata":{"id":"2vHCP-CdrzGE"},"source":["CNN Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"05UaXQgGr1kT","outputId":"fc2c5d15-9831-435c-d6be-2c9a1f42a029"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","5685/5685 [==============================] - 222s 39ms/step - loss: 0.3436 - accuracy: 0.8728 - val_loss: 0.2889 - val_accuracy: 0.8942\n","Epoch 2/10\n","5685/5685 [==============================] - 227s 40ms/step - loss: 0.2560 - accuracy: 0.9068 - val_loss: 0.2746 - val_accuracy: 0.9025\n","Epoch 3/10\n","5685/5685 [==============================] - 218s 38ms/step - loss: 0.2136 - accuracy: 0.9239 - val_loss: 0.2670 - val_accuracy: 0.9067\n","Epoch 4/10\n","5685/5685 [==============================] - 216s 38ms/step - loss: 0.1809 - accuracy: 0.9365 - val_loss: 0.2746 - val_accuracy: 0.9073\n","Epoch 5/10\n","5685/5685 [==============================] - 209s 37ms/step - loss: 0.1543 - accuracy: 0.9466 - val_loss: 0.3087 - val_accuracy: 0.9094\n","Epoch 6/10\n","5685/5685 [==============================] - 196s 35ms/step - loss: 0.1332 - accuracy: 0.9540 - val_loss: 0.3092 - val_accuracy: 0.9044\n","Epoch 7/10\n","5685/5685 [==============================] - 199s 35ms/step - loss: 0.1155 - accuracy: 0.9605 - val_loss: 0.3107 - val_accuracy: 0.9007\n","Epoch 8/10\n","5685/5685 [==============================] - 198s 35ms/step - loss: 0.1010 - accuracy: 0.9654 - val_loss: 0.3278 - val_accuracy: 0.9084\n","Epoch 9/10\n","5685/5685 [==============================] - 197s 35ms/step - loss: 0.0900 - accuracy: 0.9691 - val_loss: 0.3586 - val_accuracy: 0.9042\n","Epoch 10/10\n","5685/5685 [==============================] - 196s 34ms/step - loss: 0.0798 - accuracy: 0.9726 - val_loss: 0.3605 - val_accuracy: 0.9079\n","3553/3553 [==============================] - 18s 5ms/step\n","{'accuracy': 0.909377171455964, 'precision': 0.9060936318842626, 'recall': 0.909377171455964, 'f1': 0.9075020679476528}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Dense, GlobalMaxPooling1D\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import keras\n","\n","\n","\n","#Preparing the data for training\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(train_df['Text'])\n","\n","\n","X_train = tokenizer.texts_to_sequences(train_df['Text'])\n","X_train = pad_sequences(X_train, maxlen=100)\n","y_train = pd.get_dummies(train_df['Label']).values\n","\n","\n","X_val = tokenizer.texts_to_sequences(val_df['Text'])\n","X_val = pad_sequences(X_val, maxlen=100)\n","y_val = pd.get_dummies(val_df['Label']).values\n","\n","\n","X_test = tokenizer.texts_to_sequences(test_df['Text'])\n","X_test = pad_sequences(X_test, maxlen=100)\n","y_test = pd.get_dummies(test_df['Label']).values\n","\n","\n","#Building the CNN model\n","model = Sequential()\n","model.add(Embedding(input_dim=5000, output_dim=32, input_length=100))\n","model.add(Conv1D(filters=32, kernel_size=10, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","\n","\n","model.add(Conv1D(filters=64, kernel_size=10, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(GlobalMaxPooling1D())\n","\n","\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(3, activation='softmax'))\n","\n","\n","optimizer = keras.optimizers.Adam(learning_rate=0.001)\n","model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","\n","\n","\n","\n","#Training the model\n","history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))\n","\n","\n","#Evaluating the model\n","y_pred = model.predict(X_test)\n","y_pred = np.argmax(y_pred, axis=1)\n","y_test = np.argmax(y_test, axis=1)\n","scores = evaluate(y_test, y_pred)\n","print(scores)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"atrf1M_yr13l"},"source":["Custom **LSTM** Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":88049,"status":"error","timestamp":1682549511352,"user":{"displayName":"Praveen Raj U S","userId":"06499484797460654824"},"user_tz":240},"id":"jbXFEabBr7Ua","outputId":"bcd7362d-946e-465a-a418-bfcc6f790a1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["(568454, 10)\n","(568454, 10)\n","Epoch 1/10\n","  21/5685 [..............................] - ETA: 42:43 - loss: -0.6822 - accuracy: 0.0707"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-459ac7daf446>\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m#Fitting the LSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#Imports\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import nltk\n","import os\n","#from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import LSTM\n","from keras.layers import LSTM, Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","os.system(\"pip3 install --upgrade --no-cache-dir gdown\")\n","if not os.path.exists(\"Reviews.csv\"):\n","    os.system(\"gdown 1_kLSwiRYtiXF7h9V1FlTqTapOHiYU5Mk\")\n","\n","\n","\n","df = pd.read_csv('Reviews.csv')\n","print(df.shape)\n","df_head = df.head(1000)\n","print(df.shape)\n","\n","\n","\n","df = df[['Text', 'Score']]\n","\n","\n","df.dropna(inplace=True)\n","\n","\n","#Maping the Scores to binary labels:\n","df['Label'] = df['Score'].apply(lambda x: 'positive' if x >= 4 else 'negative' if x <= 2 else 'neutral')\n","\n","\n","\n","#Dataset Splitting:\n","from sklearn.model_selection import train_test_split\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n","\n","\n","#Evaluation Metrics:\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","\n","def evaluate(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average=\"weighted\")\n","    recall = recall_score(y_true, y_pred, average=\"weighted\")\n","    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n","    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","\n","\n","##Tokenizing the sentences\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train_df['Text'])\n","train_sequences = tokenizer.texts_to_sequences(train_df['Text'])\n","val_sequences = tokenizer.texts_to_sequences(val_df['Text'])\n","\n","\n","# Padding sequences\n","max_sequence_len = 100\n","train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n","val_sequences = pad_sequences(val_sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n","\n","\n","#Integer encoding of labels\n","label_encoder = LabelEncoder()\n","label_encoder.fit(train_df['Label'])\n","train_labels = label_encoder.transform(train_df['Label'])\n","val_labels = label_encoder.transform(val_df['Label'])\n","\n","\n","#Defining the LSTM model\n","lstm_model = Sequential()\n","lstm_model.add(Embedding(len(tokenizer.word_index)+1, 100, input_length=max_sequence_len))\n","lstm_model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n","lstm_model.add(Dense(1, activation='sigmoid'))\n","lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","#Defining callbacks\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n","model_checkpoint = ModelCheckpoint('lstm_model.h5', save_best_only=True)\n","\n","\n","#Fitting the LSTM model\n","lstm_model.fit(train_sequences, train_labels, validation_data=(val_sequences, val_labels), epochs=10, batch_size=64)\n","\n","\n","#Loading the best model\n","lstm_model.load_weights('lstm_model.h5')\n","\n","val_pred = lstm_model.predict_classes(val_sequences)\n","val_pred = val_pred.ravel()\n","precision = precision_score(val_labels, val_pred)\n","recall = recall_score(val_labels, val_pred)\n","f1 = f1_score(val_labels, val_pred)\n","\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1 score:\", f1)\n"]},{"cell_type":"markdown","metadata":{"id":"yn7oX4ofsGUb"},"source":["**LSTM** with **GloVe** Word Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1oKCrLQZsMmJ"},"outputs":[],"source":["#Imports\n","import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VPNhJ2qLsNba"},"outputs":[],"source":["#Downloading the dataset and pre-trained embeddings (GloVe)\n","os.system(\"pip3 install --upgrade --no-cache-dir gdown\")\n","if not os.path.exists(\"Reviews.csv\"):\n","    os.system(\"gdown 1_kLSwiRYtiXF7h9V1FlTqTapOHiYU5Mk\")\n","\n","if not os.path.exists(\"glove.6B.100d.txt\"):\n","    os.system(\"gdown https://nlp.stanford.edu/data/glove.6B.zip\")\n","    os.system(\"unzip glove.6B.zip glove.6B.100d.txt\")\n","\n","#Loading the dataset\n","df = pd.read_csv('Reviews.csv')\n","print(df.shape)\n","\n","#Selecting a subset of the data (to speed up training)\n","df = df.sample(frac=0.1, random_state=42)\n","\n","#Extracting only the relevant columns\n","df = df[['Text', 'Score']]\n","df.dropna(inplace=True)\n","\n","#Mapping the Scores to binary labels:\n","df['Label'] = df['Score'].apply(lambda x: 'positive' if x >= 4 else 'negative' if x <= 2 else 'neutral')\n","\n","#Dataset Splittinig\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sealicKmsTOO"},"outputs":[],"source":["#Tokenizing the sentences\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train_df['Text'])\n","train_sequences = tokenizer.texts_to_sequences(train_df['Text'])\n","val_sequences = tokenizer.texts_to_sequences(val_df['Text'])\n","\n","#Padding the sequencess\n","max_sequence_len = 100\n","train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n","val_sequences = pad_sequences(val_sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n","\n","#Integer encoding the labels\n","label_encoder = LabelEncoder()\n","label_encoder.fit(train_df['Label'])\n","train_labels = label_encoder.transform(train_df['Label'])\n","val_labels = label_encoder.transform(val_df['Label'])\n","\n","#Loading pre-trained word embeddings - GloVe Word Embeddings\n","embedding_dim = 100\n","embeddings_index = {}\n","with open(\"glove.6B.100d.txt\", encoding=\"utf8\") as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype=\"float32\")\n","        embeddings_index[word] = coefs\n","\n","#Creating an embedding matrix for our vocabulary\n","vocab_size = len(tokenizer.word_index) + 1\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","for word, i in tokenizer.word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSSohKMHsWkx"},"outputs":[],"source":["#Defining the LSTM model with pre-trained word embeddings\n","\n","lstm_model = Sequential()\n","lstm_model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_sequence_len, trainable=False))\n","lstm_model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n","lstm_model.add(Dense(64, activation='relu'))\n","lstm_model.add(Dense(3, activation='softmax'))\n","\n","\n","#Compiling the model\n","lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","lstm_model.summary()\n","\n","#Training the model\n","batch_size = 64\n","epochs = 10\n","\n","history = lstm_model.fit(train_sequences, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(val_sequences, val_labels))\n","\n","#Evaluating\n","test_sequences = tokenizer.texts_to_sequences(test_df['Text'])\n","test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n","test_labels = label_encoder.transform(test_df['Label'])\n","test_loss, test_acc = lstm_model.evaluate(test_sequences, test_labels)\n","#test_loss, test_acc = lstm_model.evaluate(test_sequences, test_labels, verbose=2)\n","\n","print('Test Accuracy:', test_acc)\n","\n","# Evaluate the model on the test data\n","#test_sequences = tokenizer.texts_to_sequences(test_df['Text'])\n","#test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n","#test_labels = label_encoder.transform(test_df['Label'])\n","\n","\n","# Make predictions on the test data\n","#test_predictions = lstm_model.predict_classes(test_sequences)\n","test_predictions = np.argmax(lstm_model.predict(test_sequences), axis=-1)\n","\n","\n","# Convert the integer labels back to original labels\n","test_predictions = label_encoder.inverse_transform(test_predictions)\n","\n","# Print the evaluation metrics\n","from sklearn.metrics import classification_report\n","\n","print(\"Test Accuracy: {:.2f}%\".format(test_acc * 100))\n","print(classification_report(test_df['Label'], test_predictions))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}